{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needed imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from utils import save_checkpoint, load_checkpoint, print_and_export_examples\n",
    "from get_loader import get_loader\n",
    "from model import CNNtoRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Define the image transformations\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((356, 356)),  # Resize the image to a specific size\n",
    "            transforms.RandomCrop((299, 299)),  # Randomly crop the image\n",
    "            transforms.ToTensor(),  # Convert the image to a tensor\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalize the image tensor\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    images_path = input(\"Enter the images path (or press Enter to use the default path): \")\n",
    "    annotations_path = input(\"Enter the annotations path (or press Enter to use the default path): \")\n",
    "\n",
    "    if not images_path:\n",
    "        images_path = \"/Users/nde-la-f/Documents/Image_caption/flickr8k/images/\"\n",
    "\n",
    "    if not annotations_path:\n",
    "        annotations_path = \"/Users/nde-la-f/Documents/Image_caption/flickr8k/captions.txt\"\n",
    "\n",
    "    # Get the data loader and dataset\n",
    "    train_loader, dataset = get_loader(\n",
    "        root_folder=images_path,\n",
    "        annotation_file=annotations_path,\n",
    "        transform=transform,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    # Set CUDA benchmark for improved performance\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # Check if CUDA is available, otherwise use CPU\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    # Set flags for loading and saving models\n",
    "    load_model = False\n",
    "    save_model = True\n",
    "\n",
    "    # Hyperparameters\n",
    "    embed_size = 256  # Dimensionality of the word embedding\n",
    "    hidden_size = 256  # Number of units in the hidden state of the RNN\n",
    "    vocab_size = len(dataset.vocab)  # Size of the vocabulary\n",
    "    learning_rate = 3e-4  # Learning rate for the optimizer\n",
    "    num_epochs = 10  # Number of training epochs\n",
    "    num_layers = 1  # Number of layers in the RNN\n",
    "\n",
    "    # Create a SummaryWriter for TensorBoard visualization\n",
    "    writer = SummaryWriter(\"runs/flickr\")\n",
    "    step = 0\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])  # Ignore padding tokens in the loss calculation\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if load_model:\n",
    "        # Load the saved checkpoint\n",
    "        step = load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Initialize a list to store the training loss values\n",
    "    train_loss_values = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0  # Variable to track the total loss for the epoch\n",
    "\n",
    "        for idx, (imgs, captions) in enumerate(train_loader):\n",
    "            imgs = imgs.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            # Forward pass through the model\n",
    "            outputs = model(imgs, captions[:-1])  # We want the model to predict the end token\n",
    "            #print(outputs) \n",
    "\t\t\t\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1))\n",
    "\n",
    "            # Log the training loss in TensorBoard\n",
    "            writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n",
    "            step += 1\n",
    "\n",
    "            # Zero the gradients, perform backward pass, and update the weights\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            loss.backward()  # Perform backward pass to calculate gradients\n",
    "            optimizer.step()  # Update the weights using the gradients\n",
    "\n",
    "            # Accumulate the loss for the epoch\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Calculate the average loss for the epoch\n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "        train_loss_values.append(epoch_loss)\n",
    "\n",
    "        # Print the epoch loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Plot the training loss curve\n",
    "    plt.plot(range(1, num_epochs+1), train_loss_values)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss Curve\")\n",
    "    plt.show()\n",
    "\n",
    "    if save_model:\n",
    "        # Save the final model checkpoint\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"step\": step,\n",
    "        }\n",
    "        save_checkpoint(checkpoint)\n",
    "    \n",
    "    # Print and export examples after training\n",
    "    print_and_export_examples(model, device, dataset, num_examples=5, export_file=\"examples.txt\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nde-la-f/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/nde-la-f/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
